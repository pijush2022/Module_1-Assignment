# Wall AR System - AI-Powered Product Placement

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Last Updated](https://img.shields.io/badge/Last%20Updated-September%202025-green.svg)](#)

A state-of-the-art AI system for realistic wall segmentation and intelligent product placement in room images. Create stunning AR try-on experiences by seamlessly placing TVs, paintings, and frames on walls with photorealistic perspective, lighting, and shadows.

## ğŸ¯ Overview

This project delivers a complete end-to-end pipeline featuring:
- **ğŸ§  Advanced Wall Detection**: U-Net architecture (primary) with SAM and traditional CV fallbacks
- **ğŸ“ Smart Product Placement**: Intelligent positioning with automatic perspective correction
- **âœ¨ Photorealistic Blending**: Advanced lighting analysis, shadow generation, and seamless integration
- **ğŸ“Š Quality Assurance**: Comprehensive confidence scoring and validation metrics
- **âš¡ Performance Optimized**: Multiple model options for different speed/quality requirements

## ğŸš€ AI Model Options

Choose the best model for your needs:

### ğŸ¥‡ **U-Net Model** (Recommended - Default)
- âš¡ **3x faster inference** than SAM models
- ğŸ¯ **Optimized for walls** - purpose-built architecture
- ğŸ’¾ **Lightweight** - no large downloads required
- ğŸ”„ **Real-time ready** - suitable for interactive applications
- ğŸ› ï¸ **Hybrid approach** - combines deep learning with traditional CV
- ğŸ“± **Mobile friendly** - lower memory footprint

### ğŸ† **SAM Model** (Optional - Maximum Quality)
- ğŸ¨ **Highest accuracy** for complex architectural scenes
- ğŸŒ **Universal segmentation** - works on any object type
- ğŸ“¦ **Large download** required (~2.6GB for ViT-H)
- ğŸ–¥ï¸ **Resource intensive** - needs powerful hardware
- ğŸ”¬ **Research grade** - state-of-the-art segmentation

### âš™ï¸ **Traditional CV** (Fallback)
- ğŸš€ **Ultra-fast** processing
- ğŸ”§ **No dependencies** - pure OpenCV
- ğŸ“ **Geometric approach** - edge and corner detection
- ğŸ’¡ **Lightweight** - minimal resource usage

## ğŸ—ï¸ Architecture

The system follows a modular architecture with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Wall           â”‚    â”‚  Product         â”‚    â”‚  Image          â”‚
â”‚  Segmentation   â”‚â”€â”€â”€â–¶â”‚  Placement       â”‚â”€â”€â”€â–¶â”‚  Blending       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ SAM Model     â”‚    â”‚ â€¢ Perspective    â”‚    â”‚ â€¢ Poisson       â”‚
â”‚ â€¢ Traditional   â”‚    â”‚   Correction     â”‚    â”‚   Blending      â”‚
â”‚   CV Methods    â”‚    â”‚ â€¢ Collision      â”‚    â”‚ â€¢ Lighting      â”‚
â”‚ â€¢ Confidence    â”‚    â”‚   Detection      â”‚    â”‚   Analysis      â”‚
â”‚   Scoring       â”‚    â”‚ â€¢ Size Scaling   â”‚    â”‚ â€¢ Shadow Gen    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Architecture

```
wall-ar-system/
â”œâ”€â”€ ğŸ”§ Core System
â”‚   â”œâ”€â”€ config.py                    # Centralized configuration
â”‚   â”œâ”€â”€ wall_segmentation.py         # Multi-model wall detection
â”‚   â”œâ”€â”€ unet_segmentation.py         # U-Net implementation
â”‚   â”œâ”€â”€ product_placement.py         # Smart positioning engine
â”‚   â”œâ”€â”€ image_blending.py            # Photorealistic compositing
â”‚   â”œâ”€â”€ wall_ar_pipeline.py          # Main orchestrator
â”‚   â””â”€â”€ utils.py                     # Shared utilities
â”‚
â”œâ”€â”€ ğŸ§ª Testing & Examples
â”‚   â”œâ”€â”€ example_usage.py             # Comprehensive demos
â”‚   â”œâ”€â”€ quick_test_with_unet.py      # U-Net validation
â”‚   â”œâ”€â”€ quick_test_with_sam.py       # SAM validation
â”‚   â””â”€â”€ setup_folders.py             # Project initialization
â”‚
â”œâ”€â”€ ğŸ“¦ Dependencies
â”‚   â””â”€â”€ requirements.txt             # Python packages
â”‚
â”œâ”€â”€ ğŸ“‚ Organized Data Structure
â”‚   â”œâ”€â”€ input/                       # Source images
â”‚   â”‚   â”œâ”€â”€ walls/                   # Room photographs
â”‚   â”‚   â””â”€â”€ products/                # Product catalog
â”‚   â”‚       â”œâ”€â”€ tvs/                 # Television models
â”‚   â”‚       â”œâ”€â”€ paintings/           # Artwork collection
â”‚   â”‚       â””â”€â”€ frames/              # Photo frames
â”‚   â”œâ”€â”€ output/                      # Generated results
â”‚   â”‚   â”œâ”€â”€ results/                 # Final compositions
â”‚   â”‚   â””â”€â”€ debug/                   # Development aids
â”‚   â”œâ”€â”€ models/                      # AI model storage
â”‚   â””â”€â”€ sample_images/               # Test datasets
â”‚
â””â”€â”€ ğŸ“š Documentation
    â””â”€â”€ README.md                    # This guide
```

## ğŸš€ Quick Start

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- **Python 3.8+** (3.9+ recommended)
- **4GB+ RAM** (8GB+ for SAM models)
- **GPU optional** (CUDA-compatible for acceleration)

### Quick Installation

```bash
# 1. Clone or download the project
git clone <repository-url> wall-ar-system
cd wall-ar-system

# 2. Create virtual environment (recommended)
python -m venv wall_ar_env

# Windows
wall_ar_env\Scripts\activate

# Linux/Mac
source wall_ar_env/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Initialize project structure
python setup_folders.py

# 5. Verify installation
python quick_test_with_unet.py
```

### ğŸ” Dependency Overview

| Package | Version | Purpose |
|---------|---------|----------|
| `torch` | â‰¥2.0.0 | U-Net model & GPU acceleration |
| `torchvision` | â‰¥0.15.0 | Image transformations |
| `opencv-python` | â‰¥4.8.0 | Computer vision operations |
| `numpy` | â‰¥1.24.0 | Numerical computations |
| `pillow` | â‰¥10.0.0 | Image I/O and processing |
| `segment-anything` | â‰¥1.0 | SAM model (optional) |
| `ultralytics` | â‰¥8.0.0 | YOLO integration |
| `scikit-image` | â‰¥0.21.0 | Advanced image processing |
| `matplotlib` | â‰¥3.7.0 | Visualization and debugging |
| `scipy` | â‰¥1.11.0 | Scientific computing |
| `transformers` | â‰¥4.30.0 | Transformer models |

### Model Setup Options

#### Option A: U-Net Model (Recommended - Default)
**No additional setup required!** The system works out-of-the-box with U-Net.

```bash
# Test U-Net setup
python quick_test_with_unet.py
```

#### Option B: SAM Model (Optional - For Higher Quality)
**Choose one SAM model variant:**

```bash
# Automated download (recommended)
python download_sam_model.py

# OR manual download (choose one):
# ViT-H (best quality, ~2.6GB)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

# ViT-L (good quality, ~1.2GB)  
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth

# ViT-B (fastest, ~375MB)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
```

**Move downloaded models to the `models/` directory**

## ğŸ® Usage Examples

### ğŸš€ Quick Start - Python API

```python
from wall_ar_pipeline import WallARPipeline
from product_placement import ProductType

# Initialize with U-Net (recommended)
pipeline = WallARPipeline()

# Place a TV on living room wall
result = pipeline.process_image(
    room_image_path="input/walls/living_room.jpg",
    product_image_path="input/products/tvs/modern_tv.png", 
    product_type=ProductType.TV,
    output_path="output/results/living_room_with_tv.png"
)

print(f"âœ… Success! Confidence: {result.confidence_score:.2f}")
print(f"â±ï¸ Processing time: {result.processing_time:.2f}s")
```

### ğŸ¨ Advanced Configuration

```python
from config import config

# Customize processing settings
config["processing"].max_image_size = (1920, 1080)
config["processing"].wall_confidence_threshold = 0.7
config["placement"].default_tv_size_ratio = 0.18
config["output"].debug_mode = True

# Initialize with custom settings
pipeline = WallARPipeline()
result = pipeline.process_image(
    room_image_path="input/walls/bedroom.jpg",
    product_image_path="input/products/paintings/abstract_art.png",
    product_type=ProductType.PAINTING,
    output_path="output/results/bedroom_art.png",
    enable_shadows=True,
    enable_lighting_match=True
)
```

### ğŸ”„ Batch Processing

```python
# Process multiple images efficiently
configs = [
    {
        "room_image_path": "input/walls/living_room.jpg",
        "product_image_path": "input/products/tvs/smart_tv.png",
        "product_type": ProductType.TV,
        "output_path": "output/results/living_room_tv.png"
    },
    {
        "room_image_path": "input/walls/bedroom.jpg", 
        "product_image_path": "input/products/paintings/landscape.png",
        "product_type": ProductType.PAINTING,
        "output_path": "output/results/bedroom_painting.png"
    }
]

stats = pipeline.batch_process(configs)
print(f"Processed {stats.total_images} images in {stats.total_time:.2f}s")
```

### ğŸ’» Command Line Interface

```bash
# Basic TV placement with U-Net
python wall_ar_pipeline.py \
    --room input/walls/living_room.jpg \
    --product input/products/tvs/modern_tv.png \
    --type tv \
    --output output/results/result.png \
    --debug

# Painting placement with custom settings
python wall_ar_pipeline.py \
    --room input/walls/bedroom.jpg \
    --product input/products/paintings/abstract_art.png \
    --type painting \
    --output output/results/bedroom_art.png \
    --size-ratio 0.15 \
    --enable-shadows

# High-quality processing with SAM
python wall_ar_pipeline.py \
    --room input/walls/office.jpg \
    --product input/products/frames/certificate.png \
    --type frame \
    --output output/results/office_frame.png \
    --sam-checkpoint models/sam_vit_h_4b8939.pth \
    --quality high
```

### ğŸ§ª Testing & Validation

```bash
# Quick validation (recommended first step)
python quick_test_with_unet.py

# SAM model testing (optional)
python quick_test_with_sam.py

# Performance benchmarking
python example_usage.py --benchmark

# Custom image testing
python wall_ar_pipeline.py \
    --room your_room.jpg \
    --product your_product.png \
    --type tv \
    --output test_result.png
```

## ğŸ¯ Product Types & Specifications

### ğŸ“º TVs (`ProductType.TV`)
- **Optimal Aspect Ratio**: 16:9 (modern displays)
- **Default Size**: 15% of wall width
- **Recommended Format**: PNG with transparent background
- **Special Features**: Screen reflection, bezel rendering, ambient lighting

### ğŸ–¼ï¸ Paintings (`ProductType.PAINTING`) 
- **Optimal Aspect Ratio**: 4:3 or 3:4 (traditional art)
- **Default Size**: 12% of wall width
- **Recommended Format**: PNG/JPG with frame included
- **Special Features**: Gallery lighting, frame shadows, matting effects

### ğŸ–¼ï¸ Frames (`ProductType.FRAME`)
- **Optimal Aspect Ratio**: 3:4 (portrait) or 4:3 (landscape)
- **Default Size**: 10% of wall width  
- **Recommended Format**: PNG with transparency
- **Special Features**: Glass reflection, matting, certificate styling

## âš™ï¸ Configuration System

The system uses a centralized configuration in `config.py` with four main categories:

### ğŸ¤– Model Configuration
```python
from config import config

# AI Model Settings
config["model"].sam_model_type = "vit_h"        # SAM variant: vit_h, vit_l, vit_b
config["model"].device = "cuda"                 # Processing device: cuda, cpu
config["model"].sam_checkpoint_path = None      # Auto-detect SAM model
```

### ğŸ–¼ï¸ Processing Configuration
```python
# Image Processing Settings
config["processing"].max_image_size = (1920, 1080)           # Max resolution
config["processing"].wall_confidence_threshold = 0.4         # Segmentation threshold
config["processing"].blend_alpha = 0.8                       # Blending strength
config["processing"].lighting_adjustment_strength = 0.3      # Lighting match intensity
```

### ğŸ“ Placement Configuration
```python
# Product Placement Settings
config["placement"].default_tv_size_ratio = 0.15             # TV size (15% of wall)
config["placement"].default_painting_size_ratio = 0.12       # Painting size (12% of wall)
config["placement"].min_wall_distance = 50                   # Edge margin (pixels)
config["placement"].perspective_correction_enabled = True    # Enable 3D correction
```

### ğŸ“¤ Output Configuration
```python
# Output & Debug Settings
config["output"].output_format = "PNG"                       # File format
config["output"].output_quality = 95                         # JPEG quality (if used)
config["output"].save_intermediate_results = False           # Save debug images
config["output"].debug_mode = False                          # Enable debug logging
```

## ğŸ“ Python Files Reference

This section provides a comprehensive overview of each Python file in the project and its specific purpose.

### Core System Files

#### `config.py`
**Purpose**: Centralized configuration management for the entire system.
- **What it does**: Defines configuration classes using dataclasses for model settings, processing parameters, placement options, and output preferences
- **Why it's needed**: Provides a single source of truth for all system settings, making it easy to adjust behavior without modifying code
- **Key components**: ModelConfig, ProcessingConfig, PlacementConfig, OutputConfig classes
- **Usage**: Import and modify settings before initializing other components

#### `unet_segmentation.py` â­ **NEW - Primary Segmentation**
**Purpose**: U-Net based wall segmentation with traditional CV enhancement.
- **What it does**: Implements lightweight U-Net architecture for semantic wall segmentation with traditional computer vision fallbacks
- **Why it's needed**: Provides faster, more reliable wall detection than SAM for this specific task
- **Key components**: UNet class (PyTorch model), UNetWallSegmenter class, traditional CV hybrid methods
- **Advantages**: 2-3x faster than SAM, no large model downloads, better integration with CV methods
- **Output**: Enhanced wall masks, confidence scores, corner detection

#### `wall_segmentation.py`
**Purpose**: Main wall segmentation orchestrator with multiple model support.
- **What it does**: Coordinates between U-Net, SAM, and traditional CV methods for wall detection
- **Why it's needed**: Provides unified interface for different segmentation approaches with automatic fallbacks
- **Key components**: WallSegmenter class, WallSegmentationResult dataclass, model switching logic
- **Model Support**: U-Net (primary), SAM (optional), Traditional CV (fallback)
- **Output**: Binary wall masks, confidence scores, wall corner coordinates

#### `product_placement.py`
**Purpose**: Intelligent product positioning with perspective correction.
- **What it does**: Places products (TVs, paintings, frames) on detected walls with proper scaling and perspective transformation
- **Why it's needed**: Ensures products look realistic and properly positioned on walls
- **Key components**: ProductPlacer class, ProductType enum, PlacementResult dataclass, perspective correction algorithms
- **Features**: Collision detection, size scaling, multiple product type support

#### `image_blending.py`
**Purpose**: Advanced image compositing with realistic lighting effects.
- **What it does**: Seamlessly blends placed products into room scenes with lighting analysis, shadow generation, and color matching
- **Why it's needed**: Creates photorealistic results by matching lighting conditions and adding realistic shadows
- **Key components**: ImageBlender class, LightingAnalysis dataclass, Poisson blending, shadow generation
- **Advanced features**: Color temperature adjustment, lighting direction analysis

#### `wall_ar_pipeline.py`
**Purpose**: Main orchestrator that coordinates the entire processing workflow.
- **What it does**: Combines wall segmentation, product placement, and image blending into a single, easy-to-use pipeline
- **Why it's needed**: Provides a high-level interface for end-to-end processing and handles error management
- **Key components**: WallARPipeline class, batch processing, performance monitoring, CLI interface
- **Features**: Progress tracking, comprehensive error handling, statistics collection

#### `utils.py`
**Purpose**: Shared utility functions and helper methods.
- **What it does**: Provides common functionality used across multiple modules
- **Why it's needed**: Reduces code duplication and centralizes common operations
- **Key components**: Image validation, quality assessment, thumbnail generation, logging setup
- **Functions**: File format validation, image enhancement, room type detection

### Test and Example Files

#### `example_usage.py`
**Purpose**: Comprehensive demonstration of system capabilities.
- **What it does**: Shows how to use all major features with practical examples
- **Why it's needed**: Serves as both documentation and testing for different use cases
- **Examples included**: Basic placement, advanced features, batch processing, quality assessment
- **Target audience**: New users learning the system

#### `quick_test.py`
**Purpose**: Simple validation script for basic functionality.
- **What it does**: Runs a minimal test to verify the pipeline works correctly
- **Why it's needed**: Quick health check for the system after installation or changes
- **Test scope**: Basic wall segmentation and product placement without advanced features
- **Usage**: Run after setup to confirm everything is working

#### `quick_test_with_unet.py` â­ **NEW - Recommended Test**
**Purpose**: Test script specifically for U-Net model validation.
- **What it does**: Tests the complete pipeline using U-Net for wall segmentation
- **Why it's needed**: Validates U-Net integration and demonstrates recommended usage
- **Features**: U-Net model testing, shadow generation, lighting effects, debug output
- **Requirements**: No additional model downloads needed
- **Usage**: `python quick_test_with_unet.py`

#### `quick_test_working.py`
**Purpose**: Fixed version of the quick test that bypasses SAM permission issues.
- **What it does**: Tests the pipeline using traditional CV methods when SAM is unavailable
- **Why it's needed**: Ensures testing works even when SAM model has permission or availability issues
- **Differences**: Uses `checkpoint_path=None` to avoid SAM dependency
- **Use case**: Fallback testing when SAM model is not accessible

#### `quick_test_with_sam.py`
**Purpose**: Enhanced test script that specifically uses SAM model when available.
- **What it does**: Tests the pipeline with SAM enabled for higher quality segmentation
- **Why it's needed**: Validates that SAM integration works correctly and provides better results
- **Features**: SAM model loading, enhanced pipeline testing, shadow and lighting effects
- **Requirements**: SAM model checkpoint file must be available

#### `simple_test.py`
**Purpose**: Minimal test with synthetic images for basic validation.
- **What it does**: Creates simple test images programmatically and runs the pipeline
- **Why it's needed**: Provides a self-contained test that doesn't require external image files
- **Advantages**: No dependency on sample images, creates controlled test conditions
- **Use case**: Initial validation and debugging

#### `debug_test.py`
**Purpose**: Step-by-step component testing for troubleshooting.
- **What it does**: Tests each pipeline component individually to isolate issues
- **Why it's needed**: Helps identify exactly where problems occur in the pipeline
- **Testing approach**: Incremental testing of segmentation, placement, and blending
- **Output**: Detailed progress reporting and error localization

#### `test_organized_structure.py`
**Purpose**: Validates the organized folder structure functionality.
- **What it does**: Tests the pipeline using the organized input/output folder system
- **Why it's needed**: Ensures the folder-based organization works correctly
- **Test scope**: TV and painting placement using organized folder paths
- **Validation**: Confirms proper file handling and output organization

#### `test_pipeline_fixed.py`
**Purpose**: Comprehensive test with all recent fixes applied.
- **What it does**: Tests the complete pipeline with NumPy fixes and proper API calls
- **Why it's needed**: Validates that all recent bug fixes work together correctly
- **Features**: Advanced pipeline features, shadow generation, lighting matching
- **Status**: Most up-to-date test reflecting current system state

### Setup and Utility Scripts

#### `setup_folders.py`
**Purpose**: Creates the organized folder structure for inputs and outputs.
- **What it does**: Generates all necessary directories and README files for proper organization
- **Why it's needed**: Sets up the project structure automatically instead of manual creation
- **Creates**: input/walls/, input/products/tvs/, input/products/paintings/, input/products/frames/, output/results/, output/debug/
- **Includes**: Sample images and documentation for each folder

#### `copy_samples.py`
**Purpose**: Migrates existing sample images to the new organized structure.
- **What it does**: Moves images from old sample_images folder to organized input folders
- **Why it's needed**: Helps transition from old flat structure to organized folder system
- **Migration**: Automatically categorizes and moves existing sample files
- **One-time use**: Typically run once during project reorganization

#### `download_sam_model.py`
**Purpose**: Automated SAM model checkpoint downloading.
- **What it does**: Downloads SAM ViT-H model checkpoint with progress reporting and verification
- **Why it's needed**: Simplifies SAM model acquisition for users who want higher quality segmentation
- **Features**: Progress tracking, file verification, error handling, resume capability
- **Model**: Downloads the high-quality ViT-H checkpoint (~2.6GB)
- **Usage**: `python download_sam_model.py`

#### `download_unet_model.py` (Optional)
**Purpose**: Downloads pre-trained U-Net model if available.
- **What it does**: Downloads pre-trained U-Net checkpoint for enhanced performance
- **Why it's needed**: Improves U-Net accuracy with trained weights (optional)
- **Note**: System works without pre-trained U-Net using hybrid CV approach

#### `fix_sam_permissions.py`
**Purpose**: Resolves file permission issues with SAM model checkpoints.
- **What it does**: Fixes read permissions on SAM model files
- **Why it's needed**: SAM model files sometimes have restricted permissions that prevent loading
- **Solution**: Adds appropriate read permissions for all users
- **Platform**: Designed for Windows file permission system

### Configuration Files

#### `requirements.txt`
**Purpose**: Python package dependency specification.
- **What it contains**: All required Python packages with version constraints
- **Why it's needed**: Ensures consistent environment setup across different systems
- **Key packages**: 
  - `torch>=2.0.0` - PyTorch for U-Net model
  - `opencv-python>=4.8.0` - Computer vision operations
  - `segment-anything>=1.0` - SAM model (optional)
  - `scikit-image>=0.21.0` - Image processing utilities
  - `pillow>=10.0.0` - Image I/O operations
- **Installation**: `pip install -r requirements.txt`

## ğŸ“š Detailed Module Documentation

### U-Net Segmentation (`unet_segmentation.py`) â­ **Primary Method**

**Purpose**: Fast and reliable wall segmentation using U-Net architecture.

**Key Features**:
- Lightweight U-Net model for semantic segmentation
- Traditional CV enhancement and fallback
- Hybrid approach combining deep learning and classical methods
- No large model downloads required
- 2-3x faster than SAM

**Usage**:
```python
from unet_segmentation import UNetWallSegmenter

# Initialize U-Net segmenter (works out-of-the-box)
segmenter = UNetWallSegmenter(device="cpu")

# Segment wall using U-Net + traditional CV
result = segmenter.segment_wall(room_image, method="unet")
print(f"U-Net confidence: {result.confidence}")
print(f"Wall corners: {result.wall_corners}")

# Visualize results
vis = segmenter.visualize_segmentation(room_image, result)
```

**Methods**:
- `segment_wall()`: Main segmentation with method selection
- `_segment_with_unet()`: U-Net-based segmentation
- `_segment_traditional()`: Traditional CV fallback
- `visualize_segmentation()`: Debug visualizations

### Wall Segmentation (`wall_segmentation.py`) - Orchestrator

**Purpose**: Unified interface for multiple segmentation approaches.

**Key Features**:
- Multi-model support (U-Net, SAM, Traditional CV)
- Automatic fallback mechanisms
- Method selection and switching
- Unified result format

**Usage**:
```python
from wall_segmentation import WallSegmenter

# Initialize with automatic model selection
segmenter = WallSegmenter()

# Segment using best available method
result = segmenter.segment_wall(room_image, method="auto")
print(f"Wall confidence: {result.confidence}")
print(f"Wall corners: {result.wall_corners}")
```

**Methods**:
- `segment_wall()`: Main segmentation method with auto-selection
- `visualize_segmentation()`: Create debug visualizations
- `_segment_traditional()`: Traditional CV methods

### Product Placement (`product_placement.py`)

**Purpose**: Place products on walls with realistic perspective and positioning.

**Key Features**:
- Automatic perspective correction
- Intelligent size scaling based on product type
- Collision detection with furniture
- Multiple product type support (TV, painting, frame)

**Usage**:
```python
from product_placement import ProductPlacer, ProductType

placer = ProductPlacer()

result = placer.place_product(
    room_image=room_img,
    product_image=product_img,
    wall_mask=wall_mask,
    wall_corners=corners,
    product_type=ProductType.TV,
    position=(0.5, 0.4),  # Center horizontally, 40% down
    size_ratio=0.15       # 15% of wall width
)
```

**Methods**:
- `place_product()`: Main placement method
- `_apply_perspective_correction()`: Perspective transformation
- `_check_collisions()`: Furniture collision detection
- `_calculate_product_dimensions()`: Smart sizing

### Image Blending (`image_blending.py`)

**Purpose**: Seamlessly blend products into room scenes with realistic lighting.

**Key Features**:
- Poisson blending for seamless integration
- Lighting direction analysis and matching
- Realistic shadow generation
- Color temperature adjustment

**Usage**:
```python
from image_blending import ImageBlender

blender = ImageBlender()

result = blender.blend_with_lighting(
    background=room_image,
    foreground=product_image,
    position=(x, y),
    wall_mask=wall_mask,
    enable_shadows=True,
    enable_lighting_match=True
)
```

**Methods**:
- `blend_with_lighting()`: Main blending method
- `_analyze_lighting()`: Scene lighting analysis
- `_add_realistic_shadows()`: Shadow generation
- `_adjust_color_temperature()`: Color matching

### Main Pipeline (`wall_ar_pipeline.py`)

**Purpose**: Orchestrate the complete processing pipeline.

**Key Features**:
- End-to-end processing workflow
- Batch processing capabilities
- Comprehensive error handling
- Performance monitoring and statistics

**Usage**:
```python
from wall_ar_pipeline import WallARPipeline

pipeline = WallARPipeline()

# Single image processing
result = pipeline.process_image(
    room_image_path="room.jpg",
    product_image_path="product.png",
    product_type="tv",
    output_path="result.png"
)

# Batch processing
configs = [
    {"room_image_path": "room1.jpg", "product_image_path": "tv.png", "product_type": "tv"},
    {"room_image_path": "room2.jpg", "product_image_path": "painting.png", "product_type": "painting"}
]

stats = pipeline.batch_process(configs, "output_directory/")
```

## ğŸ“‚ Input Organization

The system uses an organized folder structure for easy management:

### Wall Images (`input/walls/`)
- Room photos with visible, unobstructed walls
- Supported formats: JPG, PNG, BMP, TIFF
- Recommended resolution: 800x600 or higher
- Good lighting and minimal wall clutter

### Product Images (`input/products/`)

#### TVs (`input/products/tvs/`)
- **Aspect Ratio**: 16:9 (modern TVs)
- **Default Size**: 15% of wall width
- **Format**: PNG with transparent background preferred
- **Features**: Automatic screen reflection, bezel rendering

#### Paintings (`input/products/paintings/`)
- **Aspect Ratio**: 4:3 (traditional paintings)
- **Default Size**: 12% of wall width  
- **Format**: PNG or JPG with frame included
- **Features**: Frame shadow, gallery lighting

#### Frames (`input/products/frames/`)
- **Aspect Ratio**: 3:4 (portrait orientation)
- **Default Size**: 10% of wall width
- **Format**: PNG or JPG
- **Features**: Matting effects, glass reflection

## ğŸ” Quality Assessment

The system provides comprehensive quality metrics:

### Confidence Scoring
- **Segmentation Confidence**: Wall detection accuracy (0-1)
- **Placement Confidence**: Position and collision assessment (0-1)  
- **Overall Confidence**: Weighted combination of all factors (0-1)

### Quality Factors
- Wall coverage percentage
- Perspective correction accuracy
- Lighting match quality
- Shadow realism
- Edge blending smoothness

## ğŸ› ï¸ Advanced Features

### Lighting Analysis
The system analyzes room lighting to create realistic product integration:

```python
# Automatic lighting analysis
lighting = blender._analyze_lighting(room_image, wall_mask)
print(f"Light direction: {lighting.dominant_light_direction}")
print(f"Color temperature: {lighting.color_temperature}K")
```

### Shadow Generation
Realistic shadows are generated based on lighting analysis:
- Shadow direction follows light source
- Shadow intensity varies with ambient lighting
- Perspective distortion for wall-cast shadows

### Perspective Correction
Products are automatically transformed to match wall perspective:
- Vanishing point detection
- Keystone correction
- Depth-aware scaling

## ğŸ“Š Performance Benchmarks & Optimization

### ğŸƒâ€â™‚ï¸ Speed & Resource Comparison

| Model | CPU Speed | GPU Speed | Memory | Download | Quality Score | Best For |
|-------|-----------|-----------|---------|----------|---------------|----------|
| **U-Net** | 3-8s | 1-3s | 1-2GB | None | â­â­â­â­ | Production use |
| **SAM ViT-H** | 8-20s | 3-8s | 4-8GB | 2.6GB | â­â­â­â­â­ | Research/Quality |
| **SAM ViT-L** | 6-15s | 2-6s | 3-6GB | 1.2GB | â­â­â­â­ | Balanced approach |
| **SAM ViT-B** | 4-10s | 1-4s | 2-4GB | 375MB | â­â­â­ | Resource limited |
| **Traditional CV** | 1-3s | 1-3s | <1GB | None | â­â­ | Ultra-fast fallback |

### ğŸ¯ Use Case Recommendations

#### ğŸš€ **Real-time Applications**
```python
# Optimized for speed and responsiveness
pipeline = WallARPipeline()  # Uses U-Net by default
config["processing"].max_image_size = (1280, 720)  # Reduce resolution
config["output"].debug_mode = False  # Disable debug overhead
```

#### ğŸ¨ **High-Quality Production**
```python
# Maximum quality for final outputs
pipeline = WallARPipeline(sam_checkpoint_path="models/sam_vit_h_4b8939.pth")
config["processing"].max_image_size = (1920, 1080)  # Full resolution
config["placement"].perspective_correction_enabled = True
```

#### ğŸ’» **Resource-Constrained Environments**
```python
# Minimal resource usage
from wall_segmentation import WallSegmenter
segmenter = WallSegmenter()
result = segmenter._segment_traditional(image)  # Pure CV approach
```

### Recommended Usage

#### For Real-time Applications
```python
# Use U-Net for best speed/quality balance
pipeline = WallARPipeline()  # Uses U-Net by default
```

#### For Highest Quality
```python
# Use SAM ViT-H for best results
pipeline = WallARPipeline(sam_checkpoint_path="models/sam_vit_h_4b8939.pth")
```

#### For Resource-Constrained Environments
```python
# Force traditional CV methods
from wall_segmentation import WallSegmenter
segmenter = WallSegmenter()
result = segmenter._segment_traditional(image)
```

### Optimization Tips
1. **Model Selection**:
   - **U-Net**: Best overall choice for most applications
   - **SAM**: Use only when highest quality is required
   - **Traditional CV**: Fallback for very limited resources

2. **Image preprocessing**:
   - Resize large images before processing
   - Use appropriate image formats (PNG for products with transparency)

3. **Batch processing**:
   - Process multiple images together for efficiency
   - Use progress callbacks for long operations

## ğŸ§ª Testing and Validation

### Getting Started Quickly

1. **Setup the environment:**
```bash
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
python setup_folders.py
```

2. **Test U-Net model (Recommended):**
```bash
python quick_test_with_unet.py
```

3. **Test SAM model (Optional):**
```bash
# First download SAM model
python download_sam_model.py
# Then test
python quick_test_with_sam.py
```

4. **Test traditional CV methods:**
```bash
python quick_test_working.py
```

5. **Use with your own images:**
   - Place room photos in `input/walls/`
   - Place product images in appropriate `input/products/` subfolders
   - Run the pipeline with organized paths

### Test Cases
The system should be tested with:
- Various room types (living room, bedroom, office)
- Different lighting conditions (natural, artificial, mixed)
- Multiple wall orientations and perspectives
- Various product sizes and types

### Validation Metrics
- **Segmentation Confidence**: Wall detection accuracy (0-1)
- **Placement Confidence**: Position and collision assessment (0-1)
- **Overall Confidence**: Weighted combination of all factors (0-1)
- **Processing Speed**: Time per image benchmarks
- **Memory Efficiency**: Peak memory usage tracking

## ğŸš¨ Troubleshooting Guide

### ğŸ”§ Quick Fixes

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **File Not Found** | `Image file not found` error | Run `python setup_folders.py` first |
| **Low Confidence** | Confidence < 0.7 | Normal for synthetic images; try real photos |
| **Import Errors** | `U-Net model not available` | Install: `pip install torch>=2.0.0` |
| **Memory Issues** | Out of memory crashes | Use U-Net, reduce image size, or use CPU |
| **Slow Performance** | Long processing times | Enable GPU or use U-Net model |

### ğŸ› Detailed Solutions

#### ğŸ“ **File & Setup Issues**
```bash
# Create proper folder structure
python setup_folders.py

# Verify installation
python quick_test_with_unet.py

# Check file paths
ls input/walls/     # Should contain room images
ls input/products/  # Should contain product images
```

#### ğŸ§  **Model & Performance Issues**
```python
# Force U-Net usage (fastest, most reliable)
pipeline = WallARPipeline()  # Auto-uses U-Net

# Reduce memory usage
config["processing"].max_image_size = (1280, 720)
config["model"].device = "cpu"  # Use CPU if GPU memory limited

# Enable debug mode for troubleshooting
config["output"].debug_mode = True
config["output"].save_intermediate_results = True
```

#### ğŸ¯ **Quality & Placement Issues**
```python
# Improve segmentation quality
config["processing"].wall_confidence_threshold = 0.3  # Lower threshold
config["placement"].perspective_correction_enabled = True

# Adjust product sizing
config["placement"].default_tv_size_ratio = 0.12      # Smaller TV
config["placement"].default_painting_size_ratio = 0.10 # Smaller painting
```

### ğŸ†˜ **Emergency Fallback**
If all else fails, use the minimal traditional CV approach:
```python
from wall_segmentation import WallSegmenter
segmenter = WallSegmenter()
result = segmenter._segment_traditional(image)
```

## ğŸ¤ Contributing & Support

### ğŸ“‹ **Development Setup**
```bash
# Fork and clone the repository
git clone <your-fork-url>
cd wall-ar-system

# Install development dependencies
pip install -r requirements.txt
pip install pytest black flake8  # Additional dev tools

# Run tests
python -m pytest tests/
```

### ğŸ› **Bug Reports**
When reporting issues, please include:
- Python version and OS
- Complete error traceback
- Input image specifications
- Configuration settings used
- Expected vs actual behavior

### ğŸ’¡ **Feature Requests**
We welcome suggestions for:
- New product types
- Additional AI models
- Performance optimizations
- UI/UX improvements

## ğŸ“„ License & Credits

### ğŸ“œ **License**
This project is licensed under the MIT License - see the LICENSE file for details.

### ğŸ™ **Acknowledgments**
- **Meta AI** - Segment Anything Model (SAM)
- **PyTorch Team** - Deep learning framework
- **OpenCV Community** - Computer vision library
- **Contributors** - All developers who helped improve this system

### ğŸ“š **Citations**
If you use this system in research, please cite:
```bibtex
@software{wall_ar_system,
  title={Wall AR System - AI-Powered Product Placement},
  author={Your Name},
  year={2025},
  url={https://github.com/your-repo/wall-ar-system}
}
```

---

**ğŸš€ Ready to get started?** Run `python quick_test_with_unet.py` to see the system in action!

**ğŸ“§ Questions?** Check our troubleshooting guide above or open an issue on GitHub.

**â­ Like this project?** Give us a star and share with others!

## ğŸ”§ Detailed Setup Instructions

### Complete U-Net Setup (Recommended)

**Step 1: Environment Setup**
```bash
# Create virtual environment
python -m venv wall_ar_env
wall_ar_env\Scripts\activate  # Windows
# source wall_ar_env/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

**Step 2: Project Structure**
```bash
# Create organized folder structure
python setup_folders.py
```

**Step 3: Test U-Net Model**
```bash
# Test the complete pipeline with U-Net
python quick_test_with_unet.py
```

**Expected Output:**
```
ğŸ–¼ï¸ Using: w2.jpg + tv_sample.png
ğŸ§  Using U-Net for enhanced wall segmentation
âœ… Success!
   Segmentation confidence: 0.85
   Placement confidence: 0.92
   Overall confidence: 0.88
   Processing time: 3.45s
   Output: output/results/unet_test_result.png
```

### Complete SAM Setup (Optional - High Quality)

**Step 1: Download SAM Model**
```bash
# Automated download with progress tracking
python download_sam_model.py
```

**Step 2: Verify SAM Installation**
```bash
# Test SAM model functionality
python quick_test_with_sam.py
```

**Step 3: Fix Permissions (if needed)**
```bash
# Windows: Fix file permissions if SAM fails to load
python fix_sam_permissions.py
```

**Expected SAM Output:**
```
ğŸ–¼ï¸ Using: w2.jpg + tv_sample.png
ğŸ¤– Using SAM ViT-H for high-quality wall segmentation
âœ… Success!
   Segmentation confidence: 0.92
   Placement confidence: 0.89
   Overall confidence: 0.91
   Processing time: 8.12s
   Output: output/results/sam_test_result.png
```

### Model Comparison in Practice

**U-Net Results:**
- Faster processing (3-8 seconds)
- Good segmentation quality
- Works immediately after installation
- Lower memory usage
- Better for batch processing

**SAM Results:**
- Higher segmentation accuracy
- Better handling of complex scenes
- Slower processing (8-20 seconds)
- Requires large model download
- Higher memory requirements

### Production Usage Examples

**Basic Production Pipeline:**
```python
from wall_ar_pipeline import WallARPipeline
from product_placement import ProductType

# Initialize with U-Net (recommended for production)
pipeline = WallARPipeline()

# Process single image
result = pipeline.process_image(
    room_image_path="input/walls/living_room.jpg",
    product_image_path="input/products/tvs/smart_tv.png",
    product_type=ProductType.TV,
    output_path="output/results/final_result.png",
    enable_shadows=True,
    enable_lighting_match=True
)

print(f"Processing completed with {result.confidence_score:.2f} confidence")
```

**Batch Processing:**
```python
# Process multiple images efficiently
configs = [
    {
        "room_image_path": "input/walls/room1.jpg",
        "product_image_path": "input/products/tvs/tv1.png",
        "product_type": "tv"
    },
    {
        "room_image_path": "input/walls/room2.jpg",
        "product_image_path": "input/products/paintings/art1.png",
        "product_type": "painting"
    }
]

stats = pipeline.batch_process(configs, "output/batch_results/")
print(f"Processed {stats['successful']}/{stats['total_items']} images")
```

## ğŸ“ˆ Future Enhancements

### Planned Features
- **Multi-wall detection**: Handle rooms with multiple walls
- **Furniture detection**: Advanced collision avoidance
- **Style matching**: Automatic product style adaptation
- **Real-time processing**: Optimized for video streams
- **3D room understanding**: Depth estimation integration

### Integration Possibilities
- **Web API**: REST API for cloud processing
- **Mobile SDK**: iOS/Android integration
- **AR frameworks**: ARKit/ARCore compatibility
- **E-commerce platforms**: Product visualization tools

## ğŸ“„ License and Credits

### Dependencies
- **PyTorch**: Deep learning framework for U-Net model
- **OpenCV**: Computer vision operations and traditional methods
- **Segment Anything**: Meta's segmentation model (optional)
- **Pillow**: Image processing and I/O operations
- **NumPy/SciPy**: Numerical computations
- **scikit-image**: Advanced image processing utilities

### Model Credits
- **U-Net Architecture**: Lightweight implementation for wall segmentation
- **SAM (Segment Anything Model)**: Meta AI Research (optional)
- **Traditional CV methods**: OpenCV community

### Architecture Credits
- **Modular Design**: Clean separation between segmentation, placement, and blending
- **Fallback System**: Robust handling of model availability and failures
- **Performance Optimization**: Balanced approach between speed and quality

## ğŸ¤ Contributing

### Development Setup
1. Fork the repository
2. Create a feature branch
3. Install development dependencies
4. Run tests before submitting
5. Follow code style guidelines

### Code Style
- Follow PEP 8 for Python code
- Use type hints for function signatures
- Add docstrings for all public methods
- Include unit tests for new features

### Testing New Features
```bash
# Test all models
python quick_test_with_unet.py
python quick_test_with_sam.py  # If SAM available
python quick_test_working.py

# Test specific components
python debug_test.py
python test_pipeline_fixed.py
```

## ğŸ“ Support

For issues and questions:
1. **Check the troubleshooting section** above
2. **Test with different models**:
   - Try U-Net: `python quick_test_with_unet.py`
   - Try traditional CV: `python quick_test_working.py`
3. **Enable debug mode** for detailed logs:
   ```python
   from config import config
   config["output"].debug_mode = True
   ```
4. **Create detailed issue reports** with:
   - Sample images (room and product)
   - Error messages and logs
   - System specifications
   - Model used (U-Net/SAM/Traditional CV)

## ğŸ¯ Quick Start Summary

### For New Users (Recommended Path)
```bash
# 1. Setup environment
pip install -r requirements.txt
python setup_folders.py

# 2. Test U-Net model (works immediately)
python quick_test_with_unet.py

# 3. Use with your images
python wall_ar_pipeline.py --room input/walls/your_room.jpg --product input/products/tvs/your_tv.png --type tv --output output/results/result.png
```

### For Advanced Users (SAM Model)
```bash
# 1. Download SAM model (~2.6GB)
python download_sam_model.py

# 2. Test SAM functionality
python quick_test_with_sam.py

# 3. Use SAM in pipeline
python wall_ar_pipeline.py --room input/walls/room.jpg --product input/products/tvs/tv.png --type tv --output output/results/result.png --sam-checkpoint models/sam_vit_h_4b8939.pth
```

### Key Differences
- **U-Net**: Fast, reliable, works out-of-the-box
- **SAM**: Higher quality, requires download, slower processing
- **Traditional CV**: Fastest, lowest quality, always available as fallback

---

**Note**: This system is designed for both research and production use. The U-Net model provides an excellent balance of speed and quality for most applications, while SAM is available for scenarios requiring the highest possible accuracy.
